{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cond_gan_mnist_success.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "izI8Tcx5Sswy"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('device avaliable : ',device_name)\n",
        "with tf.device(device_name):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Vkit2dTCvS"
      },
      "source": [
        "import os, keras, numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from keras.datasets import mnist\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13b25edNTGV5"
      },
      "source": [
        "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
        "\n",
        "  in_label = Input(shape=(1,), name = \"input_label\")\n",
        "  li = Embedding(n_classes, 50)(in_label)\n",
        "  li = Dense(in_shape[0] * in_shape[1])(li)\n",
        "  li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\n",
        "  in_image = Input(shape=in_shape, name = \"input_image\")\n",
        "\n",
        "  merge = Concatenate()([in_image, li])\n",
        "\n",
        "  fe = Conv2D(128, (3,3), strides=(2,2), padding='same', activation = 'relu')(merge)\n",
        "  # fe = LeakyReLU(alpha=0.2)(fe)\n",
        "  fe = Conv2D(128, (3,3), strides=(2,2), padding='same', activation = 'relu')(fe)\n",
        "  # fe = LeakyReLU(alpha=0.2)(fe)\n",
        "  fe = Flatten()(fe)\n",
        "  fe = Dropout(0.2)(fe)\n",
        "  out_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\n",
        "  model = Model([in_image, in_label], out_layer)\n",
        "\n",
        "  opt = Adam(learning_rate= 3e-4, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model\n",
        "a = define_discriminator()\n",
        "a.summary()\n",
        "#keras.utils.plot_model(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GfMsg2WTJUK"
      },
      "source": [
        "def define_generator(latent_dim = 100, n_classes=10):\n",
        "\n",
        "  in_label = Input(shape=(1,))\n",
        "  li = Embedding(n_classes, 50)(in_label)\n",
        "  li = Dense(7 * 7)(li)\n",
        "  li = Reshape((7, 7, 1))(li)\n",
        "\n",
        "  in_lat = Input(shape=(latent_dim,))\n",
        "\n",
        "  n_nodes = 128 * 7 * 7\n",
        "  gen = Dense(n_nodes, activation = 'relu')(in_lat)\n",
        "  # gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Reshape((7, 7, 128))(gen)\n",
        "\n",
        "  merge = Concatenate()([gen, li])\n",
        "\n",
        "  gen = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', activation = 'relu')(merge)\n",
        "  # gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  gen = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', activation = 'relu')(gen)\n",
        "  # gen = LeakyReLU(alpha=0.2)(gen)\n",
        "  # gen = Conv2DTranspose(128, (3,3), strides=(1,1), padding='same', activation = 'relu')(gen)\n",
        "  out_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "\n",
        "  model = Model([in_lat, in_label], out_layer)\n",
        "  return model\n",
        "b = define_generator(100)\n",
        "b.summary()\n",
        "#keras.utils.plot_model(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmspouGiTLhn"
      },
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\n",
        "  d_model.trainable = False\n",
        "\n",
        "  gen_noise, gen_label = g_model.input\n",
        "  gen_output = g_model.output\n",
        "\n",
        "  gan_output = d_model([gen_output, gen_label])\n",
        "\n",
        "  model = Model([gen_noise, gen_label], gan_output)\n",
        "\n",
        "  opt = Adam(learning_rate= 3e-4, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt,  metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "c = define_gan(b, a)\n",
        "c.summary()\n",
        "#keras.utils.plot_model(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADMcakmWTN60"
      },
      "source": [
        "def load_real_samples():\n",
        "  # load dataset\n",
        "  (trainX, trainy), (_, _) = mnist.load_data()\n",
        "  \"\"\"trainX = trainX[:500]\n",
        "  trainy = trainy[:500]\"\"\"\n",
        "  #print(trainX.shape)\n",
        "  # expand to 3d, e.g. add channels\n",
        "  X = expand_dims(trainX, axis=-1)\n",
        "  #print(X.shape)\n",
        "  # convert from ints to floats\n",
        "  X = X.astype('float32')\n",
        "  # scale from [0,255] to [-1,1]\n",
        "  X = (X - 127.5) / 127.5\n",
        "  return [X, trainy]\n",
        "k = load_real_samples()\n",
        "k[0].shape,k[1].shape\n",
        "print(k[0][:16].shape)\n",
        "def save_plot(x_input,lebel,n=4):\n",
        "    for i in range(n*n):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.title(str(lebel[i]),fontsize=10,color=\"red\",fontweight=10,pad='2.0',backgroundcolor='yellow')\n",
        "        plt.imshow(x_input[i,:,:,0], cmap = 'gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "save_plot(k[0][:16], k[1][:16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR1tkpInTQq4"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "\timages, labels = dataset\n",
        "\tix = randint(0, images.shape[0], n_samples)\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn [X, labels], y\n",
        "d = generate_real_samples(k, 32)\n",
        "d[0][0].shape, d[0][1].shape, d[1].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv9g2IJXTSpw"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  z_input = x_input.reshape(n_samples, latent_dim)\n",
        "  labels = randint(0, n_classes, n_samples)\n",
        "  return [z_input, labels]\n",
        "p = generate_latent_points(100, 32)\n",
        "print(p[0].shape,p[1].shape)\n",
        " \n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y\n",
        "with tf.device(device_name):\n",
        "  kh = generate_fake_samples(b, 100, 32)\n",
        "  print(kh[0][0].shape, kh[0][1].shape, kh[1].shape)\n",
        "  #kh[0][0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIqxIVSNTUuQ"
      },
      "source": [
        "def save_plot(x_input,lebel,n=4):\n",
        "    for i in range(n*n):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.title(str(lebel[i]),fontsize=10,color=\"red\",fontweight=10,pad='2.0',backgroundcolor='yellow')\n",
        "        plt.imshow(x_input[i,:,:,0], cmap = 'gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "def summarize_the_model(generator, latent_dim = 100):\n",
        "    latent_points, labels = generate_latent_points(latent_dim= 100, n_samples= 16)\n",
        "    X  = generator.predict([latent_points, labels])\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    X = (X + 1) / 2.0\n",
        "    save_plot(X,labels, n=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0np40qe_TWwn"
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim= 100, n_epochs=3, n_batch=128):\n",
        "\n",
        "  bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "  print(\"batch per epoch: \", bat_per_epo)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  print(\"half batch: \", half_batch)\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    d_loss_r,d_loss_f = 0.0,0.0\n",
        "    g_loss = 0.0\n",
        "    \n",
        "    for j in range(bat_per_epo):\n",
        "\n",
        "      [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "      d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "      d_loss_r += d_loss1\n",
        "\n",
        "      [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "      d_loss_f += d_loss2\n",
        "\n",
        "      [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      g_loss1,_ = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "      g_loss += g_loss1\n",
        "    print('epoch -> %d, d_loss_real = %.4f, d_loss_fake = %.4f, g_loss = %.4f' %(i+1, d_loss_r, d_loss_f, g_loss))\n",
        "    summarize_the_model(g_model)\n",
        "    model_json = g_model.to_json()\n",
        "    if i == 0:\n",
        "      if \"generator.json\":\n",
        "        os.remove(\"generator.json\")\n",
        "      with open(\"generator.json\",\"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    if g_model:\n",
        "      os.remove(\"generator.json\")\n",
        "      model_json = g_model.to_json()\n",
        "      with open(\"generator.json\",\"w\") as json_file:\n",
        "        json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMtQt5LfTY82"
      },
      "source": [
        "with tf.device(device_name):\n",
        "\n",
        "  latent_dim = 100\n",
        "  n_epochs = 30\n",
        "  d_model = define_discriminator()\n",
        "  g_model = define_generator(latent_dim)\n",
        "  gan_model = define_gan(g_model, d_model)\n",
        "  dataset = load_real_samples()\n",
        "  print('READY TO GO !!!')\n",
        "  \n",
        "  train(g_model, d_model, gan_model, dataset, n_epochs, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SU0r6g1TbSe"
      },
      "source": [
        "# # save the model\n",
        "# model_json = g_model.to_json()\n",
        "# with open(\"generator.json\",\"w\") as json_file:\n",
        "#   json_file.write(model_json)\n",
        "\n",
        "# load the model\n",
        "from keras.models import model_from_json\n",
        "json_file = open(\"generator.json\",\"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "800TPS1pg8sK"
      },
      "source": [
        "latent_dim = 100\n",
        "n_samples = 16\n",
        "z_input, labels = generate_latent_points(latent_dim, n_samples)\n",
        "print(z_input.shape, labels.shape)\n",
        "data = [z_input,labels]\n",
        "pred = loaded_model.predict(data)\n",
        "pred = (pred +1 ) / 2.0\n",
        "print(pred.shape)\n",
        "save_plot(pred,labels,n=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfwzgehQTdIE"
      },
      "source": [
        "# latent_dim = 100\n",
        "# n_samples = 16\n",
        "# x_input = randn(latent_dim * n_samples)\n",
        "# print(x_input.shape)\n",
        "# z_input = x_input.reshape(n_samples, latent_dim)\n",
        "# print(z_input.shape)\n",
        "# import numpy as np\n",
        "# n_classes = 10\n",
        "# labels = randint(0, n_classes, n_samples)\n",
        "# print(labels)\n",
        "# labels = labels.reshape(n_samples,)\n",
        "# print(labels.shape)\n",
        "# data = [z_input,labels]\n",
        "# pred = loaded_model.predict(data)\n",
        "# pred = (pred +1 ) / 2.0\n",
        "# print(pred.shape)\n",
        "# save_plot(pred,labels,n=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbLQ-xcITfWQ"
      },
      "source": [
        "# import os\n",
        "# os.remove(\"generator.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2vBQ_fQThcy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}